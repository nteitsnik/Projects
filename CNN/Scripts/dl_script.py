# -*- coding: utf-8 -*-
"""DL_scipt.py

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1yuoBaQlHa1SR3b_SMPKHz_mqhI3IXQ30
"""

import os
import random
import numpy as np
import torch
from torchvision import datasets, transforms
from torch.utils.data import DataLoader
import kagglehub
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import DataLoader, random_split
from torchsummary import summary

import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
import numpy as np
from sklearn.metrics import classification_report

'''NEU Metal Surface Defects Dataset from Scratch'''
# Download latest version
data_dir = kagglehub.dataset_download("fantacher/neu-metal-surface-defects-data")
root_dir=os.path.join(data_dir,'NEU Metal Surface Defects Data')

seed = 24
random.seed(seed)
np.random.seed(seed)
torch.manual_seed(seed)
torch.cuda.manual_seed_all(seed)
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False

g = torch.Generator().manual_seed(seed)

def seed_worker(worker_id):
    worker_seed = seed + worker_id
    np.random.seed(worker_seed)
    random.seed(worker_seed)

transform = transforms.Compose([
    transforms.Grayscale(num_output_channels=1),
    transforms.Resize((64, 64)),
    transforms.ToTensor(),
    transforms.Normalize((0.5,), (0.5,))
])


train_dataset = datasets.ImageFolder(root=os.path.join(root_dir, 'train'), transform=transform)
val_dataset   = datasets.ImageFolder(root=os.path.join(root_dir, 'valid'), transform=transform)
test_dataset  = datasets.ImageFolder(root=os.path.join(root_dir, 'test'), transform=transform)

train_loader = DataLoader(
    train_dataset, batch_size=32, shuffle=True,
    worker_init_fn=seed_worker, generator=g
)

val_loader = DataLoader(
    val_dataset, batch_size=32, shuffle=False,
    worker_init_fn=seed_worker, generator=g
)

test_loader = DataLoader(
    test_dataset, batch_size=32, shuffle=False,
    worker_init_fn=seed_worker, generator=g
)

print("Classes:", train_dataset.classes)
print("Train samples:", len(train_dataset))
print("Validation samples:", len(val_dataset))
print("Test samples:", len(test_dataset))


# ## Neural Network Architecture

class CNN(nn.Module):
    def __init__(self, num_classes, dropout_prob=0.2):
        super(CNN, self).__init__()
        self.conv1 = nn.Conv2d(1, 8, kernel_size=3, padding=1)
        self.bn1   = nn.BatchNorm2d(8)

        self.conv2 = nn.Conv2d(8, 16, kernel_size=3, padding=1)
        self.bn2   = nn.BatchNorm2d(16)

        self.conv3 = nn.Conv2d(16, 32, kernel_size=3, padding=1)
        self.bn3   = nn.BatchNorm2d(32)

        self.conv4 = nn.Conv2d(32, 64, kernel_size=3, padding=1)
        self.bn4   = nn.BatchNorm2d(64)

        self.pool = nn.MaxPool2d(2, 2)
        self.dropout = nn.Dropout(dropout_prob)

        self.fc1  = nn.Linear(64 * 4 * 4, 64)
        self.fc2  = nn.Linear(64, num_classes)

    def forward(self, x):
        x = self.pool(F.relu(self.bn1(self.conv1(x))))  # Conv -> BN -> ReLU -> Pool
        x = self.pool(F.relu(self.bn2(self.conv2(x))))
        x = self.pool(F.relu(self.bn3(self.conv3(x))))
        x = self.pool(F.relu(self.bn4(self.conv4(x))))
        x = x.view(-1, 64 * 4 * 4)
        x = self.dropout(F.relu(self.fc1(x)))
        x = self.fc2(x)
        return x


# ## Training - Validation

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = CNN(num_classes=6).to(device)

criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)


def train_model(model, train_loader, valid_loader, criterion, optimizer, epochs,model_save_name):
    train_losses = []
    val_losses = []

    best_val_loss = float('inf')
    best_model_state = None

    for epoch in range(epochs):
        model.train()
        running_loss = 0.0
        correct = 0
        total = 0

        for images, labels in train_loader:
            images, labels = images.to(device), labels.to(device)

            optimizer.zero_grad()
            outputs = model(images)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()

            running_loss += loss.item()
            _, predicted = torch.max(outputs, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()

        train_accuracy = correct / total
        train_loss = running_loss / len(train_loader)
        train_losses.append(train_loss)

        # Validation
        model.eval()
        val_correct = 0
        val_total = 0
        val_loss = 0.0
        with torch.no_grad():
            for images, labels in valid_loader:
                images, labels = images.to(device), labels.to(device)
                outputs = model(images)
                loss = criterion(outputs, labels)
                val_loss += loss.item()
                _, predicted = torch.max(outputs, 1)
                val_total += labels.size(0)
                val_correct += (predicted == labels).sum().item()

        val_accuracy = val_correct / val_total
        val_loss = val_loss / len(valid_loader)
        val_losses.append(val_loss)

        print(f"Epoch {epoch+1}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, "
              f"Train Acc: {train_accuracy:.4f}, Val Acc: {val_accuracy:.4f}")

        if val_loss < best_val_loss:
            best_val_loss = val_loss
            best_model_state = model.state_dict()

    # Save best model state
    torch.save(best_model_state, f'{model_save_name}.pth')

    return train_losses, val_losses


train_losses, val_losses = train_model(model, train_loader, val_loader, criterion, optimizer, epochs=50,model_save_name='Neu_scratch')

# Create a list of epoch numbers starting from 1
epochs = range(1, len(train_losses) + 1)

# Plotting
plt.figure(figsize=(8, 5))
plt.plot(epochs, train_losses, label='Training Loss', color='green', marker='o')
plt.plot(epochs, val_losses, label='Validation Loss', color='red', marker='o')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Training vs Validation Loss 1st dataset')
plt.legend()

# Ensure x-axis ticks are at integer epoch numbers
plt.xticks(epochs, rotation=90)

plt.show()



# Find the minimum validation loss and its epoch
min_val_loss = min(val_losses)
min_epoch = val_losses.index(min_val_loss) + 1  # +1 because epochs start at 1

print(f"Minimum validation loss: {min_val_loss:.4f} at epoch {min_epoch}")


# ## Testing

# Load best weights before testing
model.load_state_dict(torch.load('Neu_scratch.pth'))


def test_model(model, test_loader):
    model.eval()
    correct = 0
    total = 0
    with torch.no_grad():
        for images, labels in test_loader:
            images, labels = images.to(device), labels.to(device)
            outputs = model(images)
            _, predicted = torch.max(outputs, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()
    print(f"Test Accuracy: {100*correct / total:.4f}%")


test_model(model,test_loader)

summary(model, input_size=(1, 64, 64))


model.eval()
all_preds = []
all_labels = []

with torch.no_grad():
    for images, labels in test_loader:
        images = images.to(device)
        labels = labels.to(device)
        outputs = model(images)
        _, preds = torch.max(outputs, 1)
        all_preds.extend(preds.cpu().numpy())
        all_labels.extend(labels.cpu().numpy())

cm = confusion_matrix(all_labels, all_preds)
class_names = test_dataset.classes  # Automatically extracted from folder names

disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)
disp.plot(cmap=plt.cm.Greens, xticks_rotation=45)
plt.title("Confusion Matrix")
plt.show()

print(classification_report(all_labels, all_preds, target_names=class_names))




# # **Transfer Learning - 1 fc layer unfrozen**

path = kagglehub.dataset_download("sayelabualigah/x-sdd")
root_dir=os.path.join(path,'X-DDD')




seed = 24
random.seed(seed)
np.random.seed(seed)
torch.manual_seed(seed)
torch.cuda.manual_seed_all(seed)
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False

g = torch.Generator().manual_seed(seed)

def seed_worker(worker_id):
    worker_seed = seed + worker_id
    np.random.seed(worker_seed)
    random.seed(worker_seed)


transform = transforms.Compose([
    transforms.Grayscale(num_output_channels=1),
    transforms.Resize((64, 64)),
    transforms.ToTensor(),
    transforms.Normalize((0.5,), (0.5,))
])

# Load entire dataset
full_dataset = datasets.ImageFolder(root_dir, transform=transform)

# Split into train(70%), val(15%), test(15%)
total_size = len(full_dataset)
train_size = int(0.7 * total_size)
val_size = int(0.15 * total_size)
test_size = total_size - train_size - val_size

generator = torch.Generator().manual_seed(seed)
train_dataset, val_dataset, test_dataset = random_split(
    full_dataset, [train_size, val_size, test_size], generator=generator
)

new_train_loader = DataLoader(
    train_dataset, batch_size=32, shuffle=True,
    worker_init_fn=seed_worker, generator=g
)

new_val_loader = DataLoader(
    val_dataset, batch_size=32, shuffle=False,
    worker_init_fn=seed_worker, generator=g
)

new_test_loader = DataLoader(
    test_dataset, batch_size=32, shuffle=False,
    worker_init_fn=seed_worker, generator=g
)

print(f"Train size: {train_size}, Val size: {val_size}, Test size: {test_size}")
print(f"Classes: {full_dataset.classes}")


# ## Neural Network Architecture

for name, param in model.named_parameters():
    if "fc2" not in name:  # freeze all except the last layer
        param.requires_grad = False
    else:
        param.requires_grad = True

num_new_classes = 7


model.fc2 = nn.Linear(64, num_new_classes).to(device)

# Ensure only this new layer's params require gradients
for param in model.fc2.parameters():
    param.requires_grad = True


# ## Training - Validation

# Ensuring that if the dataset is imbalanced will be fixed
original_dataset = new_train_loader.dataset.dataset
class_names = original_dataset.classes
num_classes = len(class_names)

# Calculate class counts from the training subset
class_counts = [0] * num_classes
for _, label in new_train_loader:
    # The labels in the DataLoader correspond to the indices in the original dataset's classes
    for l in label:
        class_counts[l] += 1


total_count = sum(class_counts)
# Handle cases where a class might have zero samples in the training split
class_weights = [total_count / class_count if class_count > 0 else 0 for class_count in class_counts]

# Ensure class weights are positive for classes with samples
class_weights = [weight if weight > 0 else 1.0 for weight in class_weights] # Assign a small weight or 1 to empty classes if needed

class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)

# Use these weights with the loss function
criterion_imbalanced = nn.CrossEntropyLoss(weight=class_weights)

print("Class Counts:", class_counts)
print("Calculated Class Weights:", class_weights)

optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=0.002)
train_losses_SDD, val_losses_SDD=train_model(model, new_train_loader, new_val_loader, criterion_imbalanced, optimizer, epochs=50,model_save_name='SDD_Tr_1lyr')

epochs = range(1, len(train_losses_SDD) + 1)

# Plotting
plt.figure(figsize=(8, 5))
plt.plot(epochs, train_losses_SDD, label='Training Loss', color='blue', marker='o')
plt.plot(epochs, val_losses_SDD, label='Validation Loss', color='pink', marker='o')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Training vs Validation Loss 2nd dataset with Transfer Learning')
plt.legend()

# Ensure x-axis ticks are at integer epoch numbers
plt.xticks(epochs, rotation=90)


plt.show()

# Find the minimum validation loss and its epoch
min_val_loss_SDD = min(val_losses_SDD)
min_epoch_SDD_trans = val_losses_SDD.index(min_val_loss_SDD) + 1  # +1 because epochs start at 1

print(f"Minimum validation loss for the 2nd dataset with transfer learning: {min_val_loss_SDD:.4f} at epoch {min_epoch_SDD_trans}")


# ## Testing

# Load best weights before testing
model.load_state_dict(torch.load('SDD_Tr_1lyr.pth'))
test_model(model,new_test_loader)
summary(model, input_size=(1, 64, 64))

model.eval()
all_preds = []
all_labels = []

with torch.no_grad():
    for images, labels in new_test_loader:
        images = images.to(device)
        labels = labels.to(device)
        outputs = model(images)
        _, preds = torch.max(outputs, 1)
        all_preds.extend(preds.cpu().numpy())
        all_labels.extend(labels.cpu().numpy())

from sklearn.metrics import classification_report

cm = confusion_matrix(all_labels, all_preds)
class_names = full_dataset.classes  # Automatically extracted from folder names

disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)
disp.plot(cmap=plt.cm.Purples, xticks_rotation=90)
plt.title("Confusion Matrix SDD Dataset - Transfer Learning")
plt.show()

print(classification_report(all_labels, all_preds, target_names=class_names))


# # **Transfer Learning - 2 fcs layers unfrozen**

model = CNN(num_classes=6).to(device)
model.load_state_dict(torch.load('Neu_scratch.pth'))


for name, param in model.named_parameters():
    if "fc2" not in name:
        param.requires_grad = False
    elif "fc1" not in name:
        param.requires_grad = False
    else:
        param.requires_grad = True

num_new_classes = 7

model.fc1 = nn.Linear(64 * 4 * 4, 64).to(device)
model.fc2 = nn.Linear(64, num_new_classes).to(device)

# Ensure only this new layer's params require gradients
for param in model.fc2.parameters():
    param.requires_grad = True
for param in model.fc1.parameters():
    param.requires_grad = True


# ## Training - Validation

# Ensuring that if the dataset is imbalanced will be fixed
original_dataset = new_train_loader.dataset.dataset
class_names = original_dataset.classes
num_classes = len(class_names)

# Calculate class counts from the training subset
class_counts = [0] * num_classes
for _, label in new_train_loader:
    # The labels in the DataLoader correspond to the indices in the original dataset's classes
    for l in label:
        class_counts[l] += 1


total_count = sum(class_counts)
# Handle cases where a class might have zero samples in the training split
class_weights = [total_count / class_count if class_count > 0 else 0 for class_count in class_counts]

# Ensure class weights are positive for classes with samples
class_weights = [weight if weight > 0 else 1.0 for weight in class_weights] # Assign a small weight or 1 to empty classes if needed

class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)

# Use these weights with the loss function
criterion_imbalanced = nn.CrossEntropyLoss(weight=class_weights)

print("Class Counts:", class_counts)
print("Calculated Class Weights:", class_weights)

optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=0.001)

# Load new data and train as usual
train_losses_SDD, val_losses_SDD=train_model(model, new_train_loader, new_val_loader, criterion_imbalanced, optimizer, epochs=50,model_save_name='SDD_Tr_2lyr')

epochs = range(1, len(train_losses_SDD) + 1)

# Plotting
plt.figure(figsize=(8, 5))
plt.plot(epochs, train_losses_SDD, label='Training Loss', color='blue', marker='o')
plt.plot(epochs, val_losses_SDD, label='Validation Loss', color='pink', marker='o')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Training vs Validation Loss 2nd dataset with Transfer Learning')
plt.legend()
# Ensure x-axis ticks are at integer epoch numbers
plt.xticks(epochs, rotation=90)


plt.show()

# Find the minimum validation loss and its epoch
min_val_loss_SDD = min(val_losses_SDD)
min_epoch_SDD_trans = val_losses_SDD.index(min_val_loss_SDD) + 1  # +1 because epochs start at 1

print(f"Minimum validation loss for the 2nd dataset with transfer learning: {min_val_loss_SDD:.4f} at epoch {min_epoch_SDD_trans}")

model.load_state_dict(torch.load('SDD_Tr_2lyr.pth'))
test_model(model,new_test_loader)

summary(model, input_size=(1, 64, 64))

model.eval()
all_preds = []
all_labels = []

with torch.no_grad():
    for images, labels in new_test_loader:
        images = images.to(device)
        labels = labels.to(device)
        outputs = model(images)
        _, preds = torch.max(outputs, 1)
        all_preds.extend(preds.cpu().numpy())
        all_labels.extend(labels.cpu().numpy())


from sklearn.metrics import classification_report

cm = confusion_matrix(all_labels, all_preds)
class_names = full_dataset.classes  # Automatically extracted from folder names

disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)
disp.plot(cmap=plt.cm.Blues, xticks_rotation=90)
plt.title("Confusion Matrix SDD Dataset - Transfer Learning")
plt.show()

print(classification_report(all_labels, all_preds, target_names=class_names))


# # **From Scratch Learning - 2nd Dataset**


model2 = CNN(num_classes=7).to(device)
optimizer2 = optim.Adam(model2.parameters(), lr=0.0005)
train_losses_SDD_fromscratch, val_losses_SDD_fromscratch = train_model(model2, new_train_loader, new_val_loader, criterion_imbalanced, optimizer2, epochs=50,model_save_name='SDD_Scratch')

epochs = range(1, len(train_losses_SDD_fromscratch) + 1)

# Plotting
plt.figure(figsize=(8, 5))
plt.plot(epochs, train_losses_SDD_fromscratch, label='Training Loss', color='orange', marker='o')
plt.plot(epochs, val_losses_SDD_fromscratch, label='Validation Loss', color='black', marker='o')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Training vs Validation Loss 2nd dataset from scratch Learning')
plt.legend()

# Ensure x-axis ticks are at integer epoch numbers
plt.xticks(epochs, rotation=90)

plt.show()

# Find the minimum validation loss and its epoch
min_val_loss_SDD = min(val_losses_SDD_fromscratch)
min_epoch_SDD_scratch = val_losses_SDD_fromscratch.index(min_val_loss_SDD) + 1  # +1 because epochs start at 1

print(f"Minimum validation loss for the 2nd dataset with transfer learning: {min_val_loss_SDD:.4f} at epoch {min_epoch_SDD_scratch}")


# ## Testing

model2.load_state_dict(torch.load('SDD_Scratch.pth'))
test_model(model2,new_test_loader)

summary(model2, input_size=(1, 64, 64))
model2.eval()
all_preds = []
all_labels = []

with torch.no_grad():
    for images, labels in new_test_loader:
        images = images.to(device)
        labels = labels.to(device)
        outputs = model2(images)
        _, preds = torch.max(outputs, 1)
        all_preds.extend(preds.cpu().numpy())
        all_labels.extend(labels.cpu().numpy())


cm = confusion_matrix(all_labels, all_preds)
class_names = full_dataset.classes  # Automatically extracted from folder names

disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)
disp.plot(cmap=plt.cm.Reds, xticks_rotation=90)
plt.title("Confusion Matrix SDD Dataset - From scratch")
plt.show()

print(classification_report(all_labels, all_preds, target_names=class_names))